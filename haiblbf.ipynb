{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 292)\n",
      "(300, 292)\n",
      "{0, 1}\n",
      "Current Rep:  0\n",
      "Dataset :  real_focus\n",
      "Number of samples:  700\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2bc5e5cb21f421083aa6a9aaa9f7fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/5000], loss: -0.461158\n",
      "Epoch[1000/5000], loss: -0.941453\n",
      "Epoch[2000/5000], loss: -0.938224\n",
      "Epoch[3000/5000], loss: -0.938767\n",
      "Epoch[4000/5000], loss: -0.943260\n",
      "test loss:  tensor(1.2155, dtype=torch.float64)\n",
      "hamming loss: 0.23666666666666666\n",
      "total reward:  tensor(229., dtype=torch.float64)\n",
      "auc:  0.7339846252402306\n",
      "IPS = 229.00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef9c61b0f1a4e12818ebff6da7f52b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/2000], loss: -0.886355\n",
      "Epoch[1000/2000], loss: -0.946903\n",
      "fraction:\n",
      "tensor(0.9967)\n",
      "Pure Alg Reward:\n",
      "tensor(229., dtype=torch.float64)\n",
      "Pure Human Reward:\n",
      "tensor(232.0000)\n",
      "test loss:  tensor(1.2155, dtype=torch.float64)\n",
      "hamming loss: 0.23666666666666666\n",
      "total reward:  tensor(227.9500)\n",
      "auc:  0.7339846252402306\n",
      "TS Rev = 227.95\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55bb5c5a97648729e5e7065bee5192e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/5000], loss: -0.551704\n",
      "Epoch[1000/5000], loss: -0.921517\n",
      "Epoch[2000/5000], loss: -0.915742\n",
      "Epoch[3000/5000], loss: -0.917560\n",
      "Epoch[4000/5000], loss: -0.920954\n",
      "fraction:\n",
      "tensor(0.7733)\n",
      "Pure Alg Reward:\n",
      "tensor(223., dtype=torch.float64)\n",
      "Pure Human Reward:\n",
      "tensor(232.0000)\n",
      "test loss:  tensor(1.5551, dtype=torch.float64)\n",
      "hamming loss: 0.25666666666666665\n",
      "total reward:  tensor(242.6000)\n",
      "auc:  0.5117931395958302\n",
      "JC Rev = 242.60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b0a82ac19b4224946219cf34316fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/5000], loss: -0.782421\n",
      "Epoch[1000/5000], loss: -1.103883\n",
      "Epoch[2000/5000], loss: -1.108832\n",
      "Epoch[3000/5000], loss: -1.151796\n",
      "Epoch[4000/5000], loss: -1.090226\n",
      "tensor([[5.6720e-05, 1.3421e-04, 4.5004e-03, 7.1101e-05, 3.9290e-05, 9.9520e-01],\n",
      "        [5.7208e-06, 1.5031e-06, 4.9327e-03, 7.4361e-06, 3.7388e-06, 9.9505e-01],\n",
      "        [2.1834e-03, 6.7432e-02, 7.4024e-01, 2.6630e-03, 1.5790e-03, 1.8590e-01],\n",
      "        ...,\n",
      "        [5.8799e-04, 2.4273e-02, 5.4531e-02, 7.3930e-04, 4.0500e-04, 9.1946e-01],\n",
      "        [3.0561e-06, 2.8328e-04, 6.4126e-07, 3.8062e-06, 2.0579e-06, 9.9971e-01],\n",
      "        [2.5589e-06, 1.1448e-03, 9.9885e-01, 3.2642e-06, 1.8055e-06, 7.1602e-08]])\n",
      "0 121 101 0 0\n",
      "test loss:  tensor(1.4211, dtype=torch.float64)\n",
      "hamming loss: 0.25666666666666665\n",
      "total reward:  tensor(257.9000, dtype=torch.float64)\n",
      "auc:  0.5033923475627512\n",
      "JCP Rev = 257.90\n",
      "(700, 292)\n",
      "(300, 292)\n",
      "{0, 1}\n",
      "Current Rep:  1\n",
      "Dataset :  <torch.utils.data.dataset.TensorDataset object at 0x7f3562184e48>\n",
      "Number of samples:  700\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e95f7e2e9a440db8164033815376a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/5000], loss: -0.591901\n",
      "Epoch[1000/5000], loss: -0.939943\n",
      "Epoch[2000/5000], loss: -0.952745\n",
      "Epoch[3000/5000], loss: -0.938183\n",
      "Epoch[4000/5000], loss: -0.942530\n",
      "test loss:  tensor(1.3080, dtype=torch.float64)\n",
      "hamming loss: 0.2633333333333333\n",
      "total reward:  tensor(221., dtype=torch.float64)\n",
      "auc:  0.6984304932735426\n",
      "IPS = 221.00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7d0978c6e946f7bcf6308531714832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/2000], loss: -0.891462\n",
      "Epoch[1000/2000], loss: -0.939814\n",
      "fraction:\n",
      "tensor(0.9967)\n",
      "Pure Alg Reward:\n",
      "tensor(221., dtype=torch.float64)\n",
      "Pure Human Reward:\n",
      "tensor(238.0000)\n",
      "test loss:  tensor(1.3080, dtype=torch.float64)\n",
      "hamming loss: 0.2633333333333333\n",
      "total reward:  tensor(219.9500)\n",
      "auc:  0.6984304932735426\n",
      "TS Rev = 219.95\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc33427fc57847e7abe5d5ed0addeb21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/5000], loss: -0.562037\n",
      "Epoch[1000/5000], loss: -0.921865\n",
      "Epoch[2000/5000], loss: -0.916219\n",
      "Epoch[3000/5000], loss: -0.912456\n",
      "Epoch[4000/5000], loss: -0.916698\n",
      "fraction:\n",
      "tensor(0.7800)\n",
      "Pure Alg Reward:\n",
      "tensor(223., dtype=torch.float64)\n",
      "Pure Human Reward:\n",
      "tensor(238.0000)\n",
      "test loss:  tensor(1.5594, dtype=torch.float64)\n",
      "hamming loss: 0.25666666666666665\n",
      "total reward:  tensor(240.7000)\n",
      "auc:  0.572287577892959\n",
      "JC Rev = 240.70\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8697e8a39817493b9afe7dbc7eaa9f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/5000], loss: -0.793020\n",
      "Epoch[1000/5000], loss: -1.086794\n",
      "Epoch[2000/5000], loss: -1.095784\n",
      "Epoch[3000/5000], loss: -1.106419\n",
      "Epoch[4000/5000], loss: -1.090041\n",
      "tensor([[9.2826e-03, 7.9566e-01, 1.6314e-01, 2.5167e-03, 1.7991e-03, 2.7602e-02],\n",
      "        [6.6391e-05, 9.9988e-01, 3.5852e-05, 8.1558e-06, 5.8976e-06, 1.2895e-06],\n",
      "        [1.2744e-02, 7.3701e-01, 2.7218e-02, 3.1335e-03, 2.2348e-03, 2.1766e-01],\n",
      "        ...,\n",
      "        [2.3925e-08, 9.9974e-01, 2.6441e-04, 1.1675e-07, 7.5272e-08, 9.5672e-09],\n",
      "        [1.7847e-06, 9.9565e-01, 1.7833e-03, 4.5969e-05, 3.1409e-05, 2.4829e-03],\n",
      "        [1.4065e-04, 9.9840e-01, 6.4111e-05, 4.7672e-05, 3.2687e-05, 1.3191e-03]])\n",
      "40 164 23 0 0\n",
      "test loss:  tensor(1.3792, dtype=torch.float64)\n",
      "hamming loss: 0.25666666666666665\n",
      "total reward:  tensor(261.6500, dtype=torch.float64)\n",
      "auc:  0.5741803040009318\n",
      "JCP Rev = 261.65\n",
      "(700, 292)\n",
      "(300, 292)\n",
      "{0, 1}\n",
      "Current Rep:  2\n",
      "Dataset :  <torch.utils.data.dataset.TensorDataset object at 0x7f356220e0f0>\n",
      "Number of samples:  700\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8127caf151b74cc7938b7915dca10aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/5000], loss: -0.541917\n",
      "Epoch[1000/5000], loss: -0.908897\n",
      "Epoch[2000/5000], loss: -0.922262\n",
      "Epoch[3000/5000], loss: -0.920890\n",
      "Epoch[4000/5000], loss: -0.916292\n",
      "test loss:  tensor(1.0704, dtype=torch.float64)\n",
      "hamming loss: 0.225\n",
      "total reward:  tensor(233., dtype=torch.float64)\n",
      "auc:  0.793252156265855\n",
      "IPS = 233.00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78e4ff36fc040af86a0e19bb2f65660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/2000], loss: -0.876266\n",
      "Epoch[1000/2000], loss: -0.932317\n",
      "fraction:\n",
      "tensor(0.9733)\n",
      "Pure Alg Reward:\n",
      "tensor(233., dtype=torch.float64)\n",
      "Pure Human Reward:\n",
      "tensor(233.0000)\n",
      "test loss:  tensor(1.0704, dtype=torch.float64)\n",
      "hamming loss: 0.225\n",
      "total reward:  tensor(228.6000)\n",
      "auc:  0.793252156265855\n",
      "TS Rev = 228.60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a5b4cd331d41e7806cfeb1365cb13a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/5000], loss: -0.564255\n",
      "Epoch[1000/5000], loss: -0.909232\n",
      "Epoch[2000/5000], loss: -0.905639\n",
      "Epoch[3000/5000], loss: -0.909308\n",
      "Epoch[4000/5000], loss: -0.904738\n",
      "fraction:\n",
      "tensor(0.7000)\n",
      "Pure Alg Reward:\n",
      "tensor(219., dtype=torch.float64)\n",
      "Pure Human Reward:\n",
      "tensor(233.0000)\n",
      "test loss:  tensor(1.6308, dtype=torch.float64)\n",
      "hamming loss: 0.27\n",
      "total reward:  tensor(243.5000)\n",
      "auc:  0.5705789503354192\n",
      "JC Rev = 243.50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0108a6c80846aba130b7896fda057b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/5000], loss: -0.790168\n",
      "Epoch[1000/5000], loss: -1.037080\n",
      "Epoch[2000/5000], loss: -1.057438\n",
      "Epoch[3000/5000], loss: -1.044019\n",
      "Epoch[4000/5000], loss: -1.032149\n",
      "tensor([[5.4488e-06, 4.1485e-05, 6.5536e-06, 7.1009e-06, 3.7495e-06, 9.9994e-01],\n",
      "        [4.3576e-06, 1.9291e-05, 5.2198e-06, 5.6466e-06, 3.0138e-06, 9.9996e-01],\n",
      "        [7.6220e-06, 9.9993e-01, 9.1155e-06, 9.7657e-06, 5.5076e-06, 4.1986e-05],\n",
      "        ...,\n",
      "        [7.8750e-06, 3.8288e-05, 9.3710e-06, 1.0108e-05, 5.5243e-06, 9.9993e-01],\n",
      "        [1.6043e-05, 9.9986e-01, 1.8938e-05, 2.0176e-05, 1.1872e-05, 6.9132e-05],\n",
      "        [4.5677e-05, 2.0713e-03, 5.4717e-05, 5.8877e-05, 3.1723e-05, 9.9774e-01]])\n",
      "0 198 0 0 0\n",
      "test loss:  tensor(1.5187, dtype=torch.float64)\n",
      "hamming loss: 0.27\n",
      "total reward:  tensor(265.1000, dtype=torch.float64)\n",
      "auc:  0.5885055527369074\n",
      "JCP Rev = 265.10\n",
      "(700, 292)\n",
      "(300, 292)\n",
      "{0, 1}\n",
      "Current Rep:  3\n",
      "Dataset :  <torch.utils.data.dataset.TensorDataset object at 0x7f35ec0b75f8>\n",
      "Number of samples:  700\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a4fbef5aef4eef8f5662b247e2978e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/5000], loss: -0.425340\n",
      "Epoch[1000/5000], loss: -0.912237\n",
      "Epoch[2000/5000], loss: -0.918009\n",
      "Epoch[3000/5000], loss: -0.924076\n",
      "Epoch[4000/5000], loss: -0.920213\n",
      "test loss:  tensor(0.9093, dtype=torch.float64)\n",
      "hamming loss: 0.20833333333333334\n",
      "total reward:  tensor(238., dtype=torch.float64)\n",
      "auc:  0.8037333333333333\n",
      "IPS = 238.00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb76195dc1a847daa2dae8fa2fae8b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/2000], loss: -0.863911\n",
      "Epoch[1000/2000], loss: -0.932407\n",
      "fraction:\n",
      "tensor(0.9767)\n",
      "Pure Alg Reward:\n",
      "tensor(238., dtype=torch.float64)\n",
      "Pure Human Reward:\n",
      "tensor(244.0000)\n",
      "test loss:  tensor(0.9093, dtype=torch.float64)\n",
      "hamming loss: 0.20833333333333334\n",
      "total reward:  tensor(236.6500)\n",
      "auc:  0.8037333333333333\n",
      "TS Rev = 236.65\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d49d00f21b8428eb9d2dc75922eaf8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/5000], loss: -0.557725\n",
      "Epoch[1000/5000], loss: -0.909039\n",
      "Epoch[2000/5000], loss: -0.899954\n",
      "Epoch[3000/5000], loss: -0.900003\n",
      "Epoch[4000/5000], loss: -0.901699\n",
      "fraction:\n",
      "tensor(0.7367)\n",
      "Pure Alg Reward:\n",
      "tensor(225., dtype=torch.float64)\n",
      "Pure Human Reward:\n",
      "tensor(244.0000)\n",
      "test loss:  tensor(1.5172, dtype=torch.float64)\n",
      "hamming loss: 0.25\n",
      "total reward:  tensor(248.0500)\n",
      "auc:  0.5569481481481482\n",
      "JC Rev = 248.05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75fbe3d2aa0545d0a1b0f43810f8ac34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/5000], loss: -0.763268\n",
      "Epoch[1000/5000], loss: -1.096096\n",
      "Epoch[2000/5000], loss: -1.092821\n",
      "Epoch[3000/5000], loss: -1.094880\n",
      "Epoch[4000/5000], loss: -1.115732\n",
      "tensor([[8.2638e-08, 9.9986e-01, 1.3732e-04, 8.7380e-07, 5.3205e-07, 4.1287e-06],\n",
      "        [1.2547e-06, 5.1805e-06, 2.6289e-07, 8.4712e-07, 5.4736e-07, 9.9999e-01],\n",
      "        [4.5870e-03, 8.4162e-01, 1.1847e-01, 1.9096e-03, 1.3036e-03, 3.2105e-02],\n",
      "        ...,\n",
      "        [2.7102e-05, 4.0929e-01, 4.0099e-06, 6.2372e-05, 4.1341e-05, 5.9057e-01],\n",
      "        [8.0811e-05, 5.1534e-03, 1.3998e-03, 1.3566e-04, 9.0400e-05, 9.9314e-01],\n",
      "        [4.3291e-03, 4.1838e-01, 2.0535e-02, 1.8477e-03, 1.2356e-03, 5.5367e-01]])\n",
      "34 76 120 0 0\n",
      "test loss:  tensor(1.3127, dtype=torch.float64)\n",
      "hamming loss: 0.25\n",
      "total reward:  tensor(254.5000, dtype=torch.float64)\n",
      "auc:  0.5658222222222222\n",
      "JCP Rev = 254.50\n",
      "(700, 292)\n",
      "(300, 292)\n",
      "{0, 1}\n",
      "Current Rep:  4\n",
      "Dataset :  <torch.utils.data.dataset.TensorDataset object at 0x7f35622bc748>\n",
      "Number of samples:  700\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f27ac7fa4e74d9bb96e9aa2dee1610f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/5000], loss: -0.612258\n",
      "Epoch[1000/5000], loss: -0.935395\n",
      "Epoch[2000/5000], loss: -0.936476\n",
      "Epoch[3000/5000], loss: -0.943802\n",
      "Epoch[4000/5000], loss: -0.941429\n",
      "test loss:  tensor(1.1858, dtype=torch.float64)\n",
      "hamming loss: 0.24\n",
      "total reward:  tensor(228., dtype=torch.float64)\n",
      "auc:  0.7541043356997972\n",
      "IPS = 228.00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4786e98b23284524ad82fc059dd3b873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/2000], loss: -0.889658\n",
      "Epoch[1000/2000], loss: -0.938252\n",
      "fraction:\n",
      "tensor(1.)\n",
      "Pure Alg Reward:\n",
      "tensor(228., dtype=torch.float64)\n",
      "Pure Human Reward:\n",
      "tensor(228.0000)\n",
      "test loss:  tensor(1.1858, dtype=torch.float64)\n",
      "hamming loss: 0.24\n",
      "total reward:  tensor(228.)\n",
      "auc:  0.7541043356997972\n",
      "TS Rev = 228.00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ca9052431049dfbcbd0bf8f6c90f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/5000], loss: -0.555091\n",
      "Epoch[1000/5000], loss: -0.916905\n",
      "Epoch[2000/5000], loss: -0.914880\n",
      "Epoch[3000/5000], loss: -0.911075\n",
      "Epoch[4000/5000], loss: -0.912373\n",
      "fraction:\n",
      "tensor(0.6867)\n",
      "Pure Alg Reward:\n",
      "tensor(232., dtype=torch.float64)\n",
      "Pure Human Reward:\n",
      "tensor(228.0000)\n",
      "test loss:  tensor(1.3529, dtype=torch.float64)\n",
      "hamming loss: 0.22666666666666666\n",
      "total reward:  tensor(245.3000)\n",
      "auc:  0.635744168356998\n",
      "JC Rev = 245.30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1404f92e36294498aaad4bc2a31cd3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/5000], loss: -0.781773\n",
      "Epoch[1000/5000], loss: -1.085147\n",
      "Epoch[2000/5000], loss: -1.073618\n",
      "Epoch[3000/5000], loss: -1.121552\n",
      "Epoch[4000/5000], loss: -1.122499\n",
      "tensor([[8.4785e-01, 1.7144e-04, 1.4979e-01, 2.0486e-03, 1.2819e-04, 1.1517e-05],\n",
      "        [1.7344e-05, 1.6820e-05, 2.3296e-05, 1.3533e-03, 1.2049e-05, 9.9858e-01],\n",
      "        [7.3045e-03, 7.8554e-05, 2.2043e-03, 2.5205e-05, 5.6427e-05, 9.9033e-01],\n",
      "        ...,\n",
      "        [1.1639e-03, 3.0730e-05, 6.6636e-06, 1.5343e-04, 2.3240e-05, 9.9862e-01],\n",
      "        [2.1276e-02, 1.9457e-05, 1.7283e-05, 9.7867e-01, 1.5086e-05, 9.5719e-07],\n",
      "        [1.1683e-05, 6.6883e-06, 2.1315e-02, 2.7043e-07, 4.6426e-06, 9.7866e-01]])\n",
      "38 0 83 78 0\n",
      "test loss:  tensor(1.2592, dtype=torch.float64)\n",
      "hamming loss: 0.22666666666666666\n",
      "total reward:  tensor(247.0500, dtype=torch.float64)\n",
      "auc:  0.5950652890466532\n",
      "JCP Rev = 247.05\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "from scipy.special import expit\n",
    "from random import seed\n",
    "from random import randrange\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "import argparse \n",
    "import torch.nn as nn \n",
    "import random\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from skmultilearn.dataset import load_dataset\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import hamming_loss, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time \n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "nrep = 5\n",
    "dataset = 'real_focus'\n",
    "hidd = 2\n",
    "rand_test = 0\n",
    "C = 0.05\n",
    "\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes, hidden = 128):\n",
    "        super(Net, self).__init__()\n",
    "        self.norm0 = torch.nn.BatchNorm1d(input_size)\n",
    "        self.linear1 = torch.nn.Linear(input_size, hidden)\n",
    "        self.linear2 = torch.nn.Linear(hidden, num_classes)\n",
    "        self.drop_layer = nn.Dropout(p=0.2)\n",
    "\n",
    "    def emb(self, x):\n",
    "        emb = self.linear1(x)\n",
    "        return emb\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        hidd1 = self.linear1(x)\n",
    "        out = self.linear2(hidd1)\n",
    "        out = self.drop_layer(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class SelNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes, hidden = 128):\n",
    "        super(SelNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_size, hidden)\n",
    "        self.linear2 = torch.nn.Linear(hidden, num_classes)\n",
    "\n",
    "    def emb(self, x):\n",
    "        emb = self.linear1(x)\n",
    "        return emb\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidd1 = F.relu(self.linear1(x))\n",
    "        out = self.linear2(hidd1)\n",
    "        return out\n",
    "\n",
    "def find_instance(w):\n",
    "    # input worker id \n",
    "    # return the instances worker labeled\n",
    "    return np.where(human[w][0].any(axis=1))[0]\n",
    "\n",
    "def find_worker(ins):\n",
    "    # input instance id \n",
    "    # return which workers labeled it\n",
    "    ws = []\n",
    "    for w in range(18):\n",
    "        if ins in  np.where(human[w][0].any(axis=1))[0]:\n",
    "            ws.append(w)\n",
    "    return np.array(ws)\n",
    "\n",
    "\n",
    "\n",
    "def find_unlabeled(human):\n",
    "    inss = []\n",
    "    for ins in range(700):\n",
    "        ws = []\n",
    "        for w in range(18):\n",
    "            if ins in  np.where(human[w][0].any(axis=1))[0]:\n",
    "                ws.append(w)\n",
    "        if len(ws) == 0:\n",
    "            inss.append(ins)\n",
    "    return np.array(inss)\n",
    "\n",
    "\n",
    "\n",
    "@ torch.no_grad()\n",
    "def test_multi(model, X_test, y_test):\n",
    "    set_seed(999)\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    out1 = model(X_test.float())\n",
    "    out = torch.sigmoid(out1)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "    _, treat = torch.max(out, 1)\n",
    "    #y = sample_y(X_test, hx_test, np.array(opt), dataset)\n",
    "    labels = y_test[range(y_test.shape[0]),treat.numpy()]\n",
    "    total_reward = labels.reshape(-1).sum()\n",
    "    pred = torch.where(out<0.5, torch.zeros_like(out), torch.ones_like(out))\n",
    "    ham_loss = hamming_loss(y_test.numpy(), pred.detach().numpy())\n",
    "    auc = roc_auc_score(y_test.numpy(), out.detach().numpy())\n",
    "    print('test loss: ', torch.nn.MultiLabelSoftMarginLoss()(out1, y_test))\n",
    "    print('hamming loss:', hamming_loss(y_test.numpy(), pred.detach().numpy()))\n",
    "    print('total reward: ', total_reward)\n",
    "    print('auc: ', auc)\n",
    "    model.train()\n",
    "    return total_reward, ham_loss, auc\n",
    "\n",
    "\n",
    "@ torch.no_grad()\n",
    "def test_multi_human(model, X_test, y_test, human, selector):\n",
    "    set_seed(999)\n",
    "    model.cpu()\n",
    "    human.cpu()\n",
    "    selector.cpu()\n",
    "\n",
    "    out1 = model(X_test.float())\n",
    "    out = torch.sigmoid(out1)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "    _, treat = torch.max(out, 1)\n",
    "\n",
    "    pred = out[range(y_test.shape[0]),treat]\n",
    "    htreat = np.argmax(human(X_test.float()).detach().cpu().numpy(),1)\n",
    "    htreat = torch.from_numpy(htreat)\n",
    "    if_alg = selector(X_test.float())\n",
    "    if_alg = torch.sigmoid(if_alg)[range(y_test.shape[0]),[0]*y_test.shape[0]]\n",
    "    treat = torch.where(if_alg > 0.5, treat, htreat)\n",
    "    #y = sample_y(X_test, hx_test, np.array(opt), dataset)\n",
    "    labels = y_test[range(y_test.shape[0]),treat.numpy()]\n",
    "    total_reward = labels.reshape(-1).sum()\n",
    "\n",
    "    pred = torch.where(out<0.5, torch.zeros_like(out), torch.ones_like(out))\n",
    "\n",
    "    ham_loss = hamming_loss(y_test.numpy(), pred.detach().numpy())\n",
    "    auc = roc_auc_score(y_test.numpy(), out.detach().numpy())\n",
    "\n",
    "    print('test loss: ', torch.nn.MultiLabelSoftMarginLoss()(out1, y_test))\n",
    "    print('hamming loss:', hamming_loss(y_test.numpy(), pred.detach().numpy()))\n",
    "    print('total reward: ', total_reward)\n",
    "    print('auc: ', auc)\n",
    "    return total_reward, ham_loss, auc\n",
    "\n",
    "@ torch.no_grad()\n",
    "def test_real_human(model, X_test, y_test, human, selector, C=0):\n",
    "    set_seed(999)\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    selector.cpu()\n",
    "\n",
    "    out1 = model(X_test.float())\n",
    "    out = torch.sigmoid(out1)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "    _, treat = torch.max(out, 1)\n",
    "    alg_treat = treat\n",
    "\n",
    "    pred = out[range(y_test.shape[0]),treat]\n",
    "    pred = out[range(y_test.shape[0]),treat]\n",
    "\n",
    "    treatment = []\n",
    "    for ins in indices_test:\n",
    "        #ws = np.array([9, 3, 11])\n",
    "        ws = np.array([0, 1, 2,3,4])\n",
    "        # randomly sample a worker\n",
    "        w = np.random.choice(ws)\n",
    "        full = human[w][ins]\n",
    "        treatment.append(randargmax(full))\n",
    "\n",
    "    htreat = np.array(treatment)\n",
    "\n",
    "    htreat = torch.from_numpy(htreat)\n",
    "    if_alg = selector(X_test.float())\n",
    "    if_alg = torch.sigmoid(if_alg)[range(y_test.shape[0]),[0]*y_test.shape[0]]\n",
    "    print('fraction:')\n",
    "    print((if_alg>0.5).float().mean())\n",
    "    treat = torch.where(if_alg > 0.5, treat, htreat)\n",
    "\n",
    "    #y = sample_y(X_test, hx_test, np.array(opt), dataset)\n",
    "    labels = y_test[range(y_test.shape[0]),treat.numpy()]\n",
    "    labels = labels.float()\n",
    "    this_reward = labels.reshape(-1)\n",
    "    this_reward = torch.where(if_alg > 0.5, labels, labels-C)\n",
    "    total_reward = this_reward.reshape(-1).sum()\n",
    "\n",
    "    print('Pure Alg Reward:')\n",
    "    labels = y_test[range(y_test.shape[0]),alg_treat.numpy()]\n",
    "    alg_reward = labels.reshape(-1).sum()\n",
    "    print(alg_reward)\n",
    "\n",
    "    print('Pure Human Reward:')\n",
    "    labels = y_test[range(y_test.shape[0]),htreat.numpy()]\n",
    "    labels = labels.float()\n",
    "    labels = labels - C\n",
    "    human_reward = labels.reshape(-1).sum()\n",
    "    print(human_reward)\n",
    "\n",
    "    pred = torch.where(out<0.5, torch.zeros_like(out), torch.ones_like(out))\n",
    "\n",
    "    ham_loss = hamming_loss(y_test.numpy(), pred.detach().numpy())\n",
    "    auc = roc_auc_score(y_test.numpy(), out.detach().numpy())\n",
    "\n",
    "    print('test loss: ', torch.nn.MultiLabelSoftMarginLoss()(out1, y_test))\n",
    "    print('hamming loss:', hamming_loss(y_test.numpy(), pred.detach().numpy()))\n",
    "    print('total reward: ', total_reward)\n",
    "    print('auc: ', auc)\n",
    "    model.train()\n",
    "    return total_reward, ham_loss, auc, human_reward\n",
    "\n",
    "@ torch.no_grad()\n",
    "def test_real_human_person(model, X_test, y_test, human, selector, C = 0):\n",
    "    set_seed(999)\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    selector.cpu()\n",
    "\n",
    "    out1 = model(X_test.float())\n",
    "    out = torch.sigmoid(out1)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "    _, treat = torch.max(out, 1)\n",
    "\n",
    "    pred = out[range(y_test.shape[0]),treat]\n",
    "    pred = out[range(y_test.shape[0]),treat]\n",
    "\n",
    "\n",
    "    if_alg = selector(X_test.float())\n",
    "    if_alg = F.softmax(if_alg, 1)\n",
    "\n",
    "    print(if_alg)\n",
    "    _, selection = torch.max(if_alg, 1)\n",
    "\n",
    "    ma = {0:0,1:1,2:2,3:3,4:4}\n",
    "    final_treat = []\n",
    "\n",
    "    treatment = []\n",
    "    treat = treat.numpy()\n",
    "    human_cost = 0 \n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    count3 = 0\n",
    "    count4 = 0\n",
    "    count5 = 0\n",
    "    for ind, ins in enumerate(indices_test):\n",
    "        i = selection[ind]\n",
    "        if i == 0:\n",
    "            fulllabel = human[0][ins]\n",
    "            treatment.append(randargmax(fulllabel))\n",
    "            human_cost += C\n",
    "            count1 += 1\n",
    "        if i == 1:\n",
    "            fulllabel = human[1][ins]\n",
    "            treatment.append(randargmax(fulllabel))\n",
    "            human_cost += C\n",
    "            count2 += 1\n",
    "        if i == 2:\n",
    "            fulllabel = human[2][ins]\n",
    "            treatment.append(randargmax(fulllabel))  \n",
    "            human_cost += C                 \n",
    "            count3 += 1\n",
    "        if i == 3:\n",
    "            fulllabel = human[3][ins]\n",
    "            treatment.append(randargmax(fulllabel))  \n",
    "            human_cost += C                 \n",
    "            count4 += 1\n",
    "        if i == 4:\n",
    "            fulllabel = human[4][ins]\n",
    "            treatment.append(randargmax(fulllabel))  \n",
    "            human_cost += C                 \n",
    "            count5 += 1\n",
    "        if i == 5:\n",
    "            treatment.append(treat[ind])                  \n",
    "    print(count1, count2, count3, count4, count5)\n",
    "    treat = np.array(treatment)\n",
    "    #y = sample_y(X_test, hx_test, np.array(opt), dataset)\n",
    "    labels = y_test[range(y_test.shape[0]),treat]\n",
    "    total_reward = labels.reshape(-1).sum() - human_cost\n",
    "\n",
    "    pred = torch.where(out<0.5, torch.zeros_like(out), torch.ones_like(out))\n",
    "\n",
    "    ham_loss = hamming_loss(y_test.numpy(), pred.detach().numpy())\n",
    "    auc = roc_auc_score(y_test.numpy(), out.detach().numpy())\n",
    "\n",
    "    print('test loss: ', torch.nn.MultiLabelSoftMarginLoss()(out1, y_test))\n",
    "    print('hamming loss:', hamming_loss(y_test.numpy(), pred.detach().numpy()))\n",
    "    print('total reward: ', total_reward)\n",
    "    print('auc: ', auc)\n",
    "    model.train()\n",
    "    return total_reward, ham_loss, auc\n",
    "\n",
    "\n",
    "\n",
    "def train_ips(trainloader, input_dim, output_dim, lamb = 0):\n",
    "    model = Net(input_dim, output_dim, hidden = hidd)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    cmcriterion = torch.nn.MultiLabelSoftMarginLoss()\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr = 1e-1, momentum=0.9, weight_decay = 1e-4)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 1e-4)\n",
    "    num_epochs = 5000\n",
    "    min_loss = 1e3\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (inputx, treatment, targety, logging_prob) in enumerate(trainloader):\n",
    "            # forward\n",
    "            out = model(inputx.float())\n",
    "            out = F.softmax(out, 1)\n",
    "            out = out[range(out.size(0)),treatment]\n",
    "            logp = logging_prob[range(out.size(0)),treatment]\n",
    "            reward = targety.reshape(-1)\n",
    "            loss = - (reward - lamb) * out / logp\n",
    "            loss = loss.mean()\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        if epoch % 1000 == 0:\n",
    "            print('Epoch[{}/{}], loss: {:.6f}'.format(epoch, num_epochs, running_loss / (batch_idx+1)))\n",
    "    return model\n",
    "\n",
    "def train_ips_2s(trainloader, input_dim, output_dim, model, lamb = 0, C = 0):\n",
    "    model = model\n",
    "    #model = Net(input_dim, output_dim, hidden = hidd)\n",
    "    selector = SelNet(input_dim, 1, hidden = 16)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    cmcriterion = torch.nn.MultiLabelSoftMarginLoss()\n",
    "    #optimizer = torch.optim.SGD(selector.parameters(), lr = 1e-2, momentum=0.9, weight_decay = 1e-4)\n",
    "    optimizer = torch.optim.Adam(selector.parameters(), lr = 1e-3, weight_decay = 1e-4)\n",
    "    num_epochs = 2000\n",
    "    min_loss = 1e3\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (inputx, treatment, targety, logging_prob) in enumerate(trainloader):\n",
    "            # forward\n",
    "            with torch.no_grad():\n",
    "                out = model(inputx.float())\n",
    "                out = F.softmax(out, 1)\n",
    "                out = out[range(out.size(0)),treatment]\n",
    "            # prob that selcts alg\n",
    "            sel_prob = selector(inputx.float())\n",
    "            sel_prob = torch.sigmoid(sel_prob).reshape(-1)\n",
    "            #print('prob')\n",
    "            #print(sel_prob)\n",
    "            logp = logging_prob[range(out.size(0)),treatment]\n",
    "            reward = targety.reshape(-1)\n",
    "            loss = - (reward - lamb) * (sel_prob * out / logp) - (1-sel_prob) * (reward - lamb - C)\n",
    "            '''\n",
    "            print('sel_prob')\n",
    "            print(sel_prob)\n",
    "            print('alg_reward')\n",
    "            print(reward *  out / logp)\n",
    "            print('human reward')\n",
    "            print(reward)\n",
    "            '''\n",
    "            loss = loss.mean()\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        if epoch % 1000 == 0:\n",
    "            print('Epoch[{}/{}], loss: {:.6f}'.format(epoch, num_epochs, running_loss / (batch_idx+1)))\n",
    "    return model, selector\n",
    "\n",
    "\n",
    "def train_ips_hai(trainloader, model, selector, input_dim, output_dim, lamb = 0, C = 0):\n",
    "    model = Net(input_dim, output_dim, hidden = hidd)\n",
    "    #selector = SelNet(input_dim, 1, hidden = 16)\n",
    "    #model = model\n",
    "    selector = selector\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    cmcriterion = torch.nn.MultiLabelSoftMarginLoss()\n",
    "    optimizer = torch.optim.Adam(list(model.parameters())+list(selector.parameters()), lr = 1e-3, weight_decay = 1e-4)\n",
    "    #optimizer = torch.optim.SGD(list(model.parameters())+list(selector.parameters()), lr = 1e-4, momentum = 0.9, weight_decay = 1e-4)\n",
    "    #optimizer = torch.optim.SGD(list(model.parameters())+list(selector.parameters()), lr = 1e-3, momentum=0.9, weight_decay = 1e-4)\n",
    "    #model_optimizer = torch.optim.SGD(list(model.parameters()), lr = 0.01, momentum=0.9, weight_decay = 1e-4)\n",
    "    #sel_optimizer = torch.optim.Adam(list(selector.parameters()), lr = 0.01, weight_decay = 1e-4)\n",
    "    num_epochs = 5000\n",
    "    min_loss = 1e3\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (inputx, treatment, targety, logging_prob) in enumerate(trainloader):\n",
    "            # forward\n",
    "            out = model(inputx.float())\n",
    "            out = F.softmax(out, 1)\n",
    "            out = out[range(out.size(0)),treatment]\n",
    "            # prob that selcts alg\n",
    "            sel_prob = selector(inputx.float())\n",
    "            sel_prob = torch.sigmoid(sel_prob).reshape(-1)\n",
    "            #print('prob')\n",
    "            #print(sel_prob)\n",
    "            logp = logging_prob[range(out.size(0)),treatment]\n",
    "            reward = targety.reshape(-1)\n",
    "            loss = - (reward - lamb) * (sel_prob * out / logp) - (1-sel_prob) * (reward - lamb - C)\n",
    "            loss = loss.mean()\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        if epoch % 1000 == 0:\n",
    "            print('Epoch[{}/{}], loss: {:.6f}'.format(epoch, num_epochs, running_loss / (batch_idx+1)))\n",
    "    return model, selector\n",
    "\n",
    "\n",
    "def train_ips_hai_person(trainloader, model, input_dim, output_dim, lamb = 0, C = 0):\n",
    "    #model = Net(input_dim, output_dim, hidden = hidd)\n",
    "    model = model\n",
    "    selector = SelNet(input_dim, 6, hidden = 16)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    cmcriterion = torch.nn.MultiLabelSoftMarginLoss()\n",
    "    optimizer = torch.optim.Adam(list(model.parameters())+list(selector.parameters()), lr = 1e-3,weight_decay=1e-4)\n",
    "    #optimizer = torch.optim.SGD(list(model.parameters())+list(selector.parameters()), lr = 1e-1, momentum=0.9, weight_decay = 1e-4)\n",
    "    #temp_optimizer = torch.optim.SGD([temperature], lr=0.001, momentum=0.9, weight_decay = 1e-4)\n",
    "    num_epochs = 5000\n",
    "    min_loss = 1e3\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (inputx, treatment, targety, logging_prob, prop, wids) in enumerate(trainloader):            # forward\n",
    "            out = model(inputx.float())\n",
    "            out = F.softmax(out, 1)\n",
    "            out = out[range(out.size(0)),treatment]\n",
    "            # prob that selcts alg\n",
    "            sel_prob = selector(inputx.float())\n",
    "            sel_prob = F.softmax(sel_prob, 1)\n",
    "            \n",
    "            logp = logging_prob\n",
    "            #logp = logging_prob[range(out.size(0)),treatment]\n",
    "            #logp = torch.sum(prop, 1) * 1 / 5\n",
    "            reward = targety.reshape(-1)\n",
    "            loss = - (reward - lamb) * (sel_prob[:,5] * out / logp) - (reward - lamb - C) * 5 * sel_prob[:,wids] \n",
    "            loss = loss.mean()\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        if epoch % 1000 == 0:\n",
    "            print('Epoch[{}/{}], loss: {:.6f}'.format(epoch, num_epochs, running_loss / (batch_idx+1)))   \n",
    "    return model, selector\n",
    "\n",
    "def randargmax(b,**kw):\n",
    "  \"\"\" a random tie-breaking argmax\"\"\"\n",
    "  return np.argmax(np.random.random(b.shape) * (b==b.max()), **kw)\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "opt_result = []\n",
    "ips_result = []\n",
    "ips_loss = []\n",
    "ips_time = []\n",
    "ips_auc = []\n",
    "ips2s_result = []\n",
    "ips2s_loss = []\n",
    "ips2s_time = []\n",
    "ips2s_auc = []\n",
    "ipsj_result = []\n",
    "ipsj_loss = []\n",
    "ipsj_time = []\n",
    "ipsj_auc = []\n",
    "ipsjp_result = []\n",
    "ipsjp_loss = []\n",
    "ipsjp_time = []\n",
    "ipsjp_auc = []\n",
    "human_result = []\n",
    "\n",
    "lamb = 0.\n",
    "#C = 0\n",
    "dataset0 = dataset\n",
    "\n",
    "for r in range(nrep):\n",
    "\n",
    "    set_seed(r)\n",
    "\n",
    "    if dataset0 == 'real_focus':\n",
    "        data = pd.read_csv('/home/ruijiang/E/utaustin/project/cost_efficient_labeling/Quick-and-Dirty/data/real/data.csv')\n",
    "        annotation = pd.read_csv('/home/ruijiang/E/utaustin/project/cost_efficient_labeling/Quick-and-Dirty/data/real/focus.csv')\n",
    "        data = data.iloc[:,4:]\n",
    "        X = data.values\n",
    "        y = annotation['gt'].values\n",
    "        a = np.zeros((X.shape[0],2))\n",
    "        a[range(a.shape[0]),y] = 1\n",
    "        y = a\n",
    "        human = dict()\n",
    "        for w in range(5):\n",
    "            temp = np.array(annotation[str(w)])\n",
    "            anno = np.zeros((X.shape[0],2))\n",
    "            anno[range(anno.shape[0]),temp] = 1\n",
    "            human[w] = anno\n",
    "        X, X_test, y, y_test, indices_train, indices_test = train_test_split(X, y, range(X.shape[0]), test_size=0.3, random_state =  r)\n",
    "        print(X.shape)\n",
    "        print(X_test.shape)\n",
    "        y_all = y\n",
    "        X_test = torch.from_numpy(X_test)\n",
    "        ws = range(5)\n",
    "    # human generate the logging policy\n",
    "    np.random.seed(r + 81)\n",
    "    treatment = []\n",
    "    wids = []\n",
    "    for ins in indices_train:\n",
    "        # randomly sample a worker\n",
    "        w = np.random.choice(ws)\n",
    "        wids.append(w)\n",
    "        full = human[w][ins]\n",
    "        treatment.append(randargmax(full))\n",
    "    treatment = np.array(treatment)\n",
    "    wids = np.array(wids)\n",
    "    #m = {3:0,9:1,11:2}\n",
    "    m = {0:0,1:1,2:2,3:3,4:4}\n",
    "    wids_map = np.array([m[i] for i in wids])\n",
    "    # record instances recorded by )\n",
    "    wids_oh = np.zeros((wids.size, wids_map.max()+1))\n",
    "    wids_oh[np.arange(wids.size),wids_map] = 1\n",
    "    y = y_all[range(y_all.shape[0]), treatment]\n",
    "\n",
    "    print(set(treatment))\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    # for ips baseline:\n",
    "    # learn logging policy \n",
    "    Xw = np.append(X,wids_oh,axis=1)\n",
    "    #log_est = LogisticRegression(random_state=0).fit(Xw, treatment)\n",
    "    #log_est0 = LogisticRegression(random_state=0).fit(X, treatment)\n",
    "    log_est = RandomForestClassifier(random_state=0).fit(Xw, treatment)\n",
    "    #log_est = MLPClassifier(random_state=0, max_iter=10000).fit(Xw, treatment)\n",
    "    #log_est = MLPClassifier(random_state=0,max_iter = 10000).fit(X, treatment)\n",
    "    log_est0 = RandomForestClassifier(random_state=0).fit(X, treatment)\n",
    "    #log_est0 = MLPClassifier(random_state=0, max_iter=10000).fit(X, treatment)\n",
    "\n",
    "\n",
    "    logging_prob = log_est0.predict_proba(X)\n",
    "    logging_prob = torch.from_numpy(logging_prob)\n",
    "\n",
    "    X = torch.from_numpy(X)\n",
    "    treatment = torch.from_numpy(treatment)\n",
    "\n",
    "    print('Current Rep: ', r)\n",
    "    print('Dataset : ', dataset)\n",
    "    print('Number of samples: ', X.shape[0])\n",
    "    #model = generate_logging_policy(X, y, frac = 0.05)\n",
    "\n",
    "    dataset = TensorDataset(X, treatment, y, logging_prob)\n",
    "    trainloader = DataLoader(dataset, batch_size = 8, shuffle = True)\n",
    "    \n",
    "    # Naive IPS\n",
    "    start = time.time()\n",
    "    ips = train_ips(trainloader, X.shape[1], y_all.shape[1], lamb = lamb)\n",
    "    end = time.time()\n",
    "    rev, _, _ = test_multi(ips, X_test, y_test)\n",
    "    print('IPS = %.2f' % rev)\n",
    "    ips_result.append(rev)\n",
    "    ips_fix = ips\n",
    "    \n",
    "    start = time.time()\n",
    "    ips, selector = train_ips_2s(trainloader, X.shape[1], y_all.shape[1], ips, lamb = lamb, C = C)\n",
    "    end = time.time()\n",
    "    rev, _, _, hreward = test_real_human(ips, X_test, y_test, human, selector, C = C)\n",
    "    print('TS Rev = %.2f' % rev)\n",
    "    ips2s_result.append(rev)\n",
    "    human_result.append(hreward)\n",
    "    \n",
    "    start = time.time()\n",
    "    ips, selector = train_ips_hai(trainloader, ips, selector, X.shape[1], y_all.shape[1], lamb = lamb, C = C)\n",
    "    end = time.time()\n",
    "    rev, _, _, _ = test_real_human(ips, X_test, y_test, human, selector, C = C)\n",
    "    print('JC Rev = %.2f' % rev)\n",
    "    ipsj_result.append(rev)\n",
    "    \n",
    "    X = X.numpy()\n",
    "    Xw = np.append(X,wids_oh,axis=1)\n",
    "    logging_prob = log_est.predict_proba(Xw)\n",
    "    logging_prob = torch.from_numpy(logging_prob)\n",
    "    X = torch.from_numpy(X)\n",
    "    w1 = np.zeros((wids.size, wids_map.max()+1))\n",
    "    w1[:,0] = 1\n",
    "    w1 = torch.from_numpy(w1)\n",
    "    w2 = np.zeros((wids.size, wids_map.max()+1))\n",
    "    w2[:,1] = 1\n",
    "    w2 = torch.from_numpy(w2)\n",
    "    w3 = np.zeros((wids.size, wids_map.max()+1))\n",
    "    w3[:,2] = 1\n",
    "    w3 = torch.from_numpy(w3)\n",
    "    w4 = np.zeros((wids.size, wids_map.max()+1))\n",
    "    w4[:,3] = 1\n",
    "    w4 = torch.from_numpy(w4)\n",
    "    w5 = np.zeros((wids.size, wids_map.max()+1))\n",
    "    w5[:,4] = 1\n",
    "    w5 = torch.from_numpy(w5)\n",
    "    prop = torch.from_numpy(np.stack([log_est.predict_proba(np.append(X,w1,axis=1))[np.arange(X.shape[0]),treatment],log_est.predict_proba(np.append(X,w2,axis=1))[np.arange(X.shape[0]),treatment],\n",
    "        log_est.predict_proba(np.append(X,w3,axis=1))[np.arange(X.shape[0]),treatment],log_est.predict_proba(np.append(X,w4,axis=1))[np.arange(X.shape[0]),treatment],log_est.predict_proba(np.append(X,w5,axis=1))[np.arange(X.shape[0]),treatment]],axis=1))\n",
    "    logging_prob = prop[range(prop.shape[0]),wids_map]\n",
    "    wids_map = torch.from_numpy(wids_map)\n",
    "    dataset = TensorDataset(X, treatment, y, logging_prob, prop, wids_map)\n",
    "    trainloader = DataLoader(dataset, batch_size = 8, shuffle = True)\n",
    "    \n",
    "    start = time.time()\n",
    "    ips, selector = train_ips_hai_person(trainloader, ips, X.shape[1], y_all.shape[1], lamb = lamb, C = C)\n",
    "    end = time.time()\n",
    "    rev, _, _ = test_real_human_person(ips, X_test, y_test, human, selector, C = C)\n",
    "    print('JCP Rev = %.2f' % rev)\n",
    "    ipsjp_result.append(rev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN:\n",
      "234.99997\n",
      "5.5136194\n",
      "IPS:\n",
      "229.8\n",
      "2.5203174403237383\n",
      "IPS-2S:\n",
      "228.23001\n",
      "2.3640893178329865\n",
      "IPS-J:\n",
      "244.02998\n",
      "1.1169239259736272\n",
      "IPS-J-P:\n",
      "257.23999999999995\n",
      "2.778539184535648\n"
     ]
    }
   ],
   "source": [
    "print('HUMAN:')\n",
    "print(np.mean(human_result))\n",
    "print(np.std(human_result))\n",
    "print('IPS:')\n",
    "print(np.mean(ips_result))\n",
    "print(np.std(ips_result)/np.sqrt(len(ips_result)))\n",
    "print('IPS-2S:')\n",
    "print(np.mean(ips2s_result))\n",
    "print(np.std(ips2s_result)/np.sqrt(len(ips2s_result)))\n",
    "print('IPS-J:')\n",
    "print(np.mean(ipsj_result))\n",
    "print(np.std(ipsj_result)/np.sqrt(len(ipsj_result)))\n",
    "print('IPS-J-P:')\n",
    "print(np.mean(ipsjp_result))\n",
    "print(np.std(ipsjp_result)/np.sqrt(len(ipsjp_result)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
