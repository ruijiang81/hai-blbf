{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 292)\n",
      "(300, 292)\n",
      "{0, 1}\n",
      "Current Rep:  0\n",
      "Dataset :  real_focus\n",
      "Number of samples:  700\n",
      "Epoch[0/5000], loss: -0.456829\n",
      "Epoch[1000/5000], loss: -0.949159\n",
      "Epoch[2000/5000], loss: -0.954586\n",
      "Epoch[3000/5000], loss: -0.940715\n",
      "Epoch[4000/5000], loss: -0.945776\n",
      "test loss:  tensor(1.4636, dtype=torch.float64)\n",
      "hamming loss: 0.2683333333333333\n",
      "total reward:  tensor(219., dtype=torch.float64)\n",
      "auc:  0.7661172907809679\n",
      "IPS = 219.00\n",
      "Epoch[0/2000], loss: -0.881051\n",
      "Epoch[1000/2000], loss: -0.952524\n",
      "fraction:\n",
      "tensor(1.)\n",
      "Pure Alg Reward:\n",
      "tensor(219., dtype=torch.float64)\n",
      "Pure Human Reward:\n",
      "tensor(231.9999)\n",
      "test loss:  tensor(1.4636, dtype=torch.float64)\n",
      "hamming loss: 0.2683333333333333\n",
      "total reward:  tensor(219.)\n",
      "auc:  0.7661172907809679\n",
      "TS Rev = 219.00\n",
      "Epoch[0/5000], loss: -0.546349\n",
      "Epoch[1000/5000], loss: -0.912815\n",
      "Epoch[2000/5000], loss: -0.910126\n",
      "Epoch[3000/5000], loss: -0.911151\n",
      "Epoch[4000/5000], loss: -0.911385\n",
      "fraction:\n",
      "tensor(0.6633)\n",
      "Pure Alg Reward:\n",
      "tensor(223., dtype=torch.float64)\n",
      "Pure Human Reward:\n",
      "tensor(231.9999)\n",
      "test loss:  tensor(1.5502, dtype=torch.float64)\n",
      "hamming loss: 0.25666666666666665\n",
      "total reward:  tensor(240.9500)\n",
      "auc:  0.5821734319492167\n",
      "JC Rev = 240.95\n",
      "Epoch[0/5000], loss: -0.783145\n",
      "Epoch[1000/5000], loss: -1.067472\n",
      "Epoch[2000/5000], loss: -1.083144\n",
      "Epoch[3000/5000], loss: -1.083755\n",
      "Epoch[4000/5000], loss: -1.092709\n",
      "tensor([[1.2178e-04, 7.4940e-05, 3.5634e-02, 1.8783e-04, 8.6210e-05, 9.6390e-01],\n",
      "        [1.1684e-05, 6.4969e-01, 1.0269e-06, 1.9959e-05, 7.0823e-06, 3.5028e-01],\n",
      "        [8.9916e-04, 3.3315e-01, 6.6220e-01, 1.3437e-03, 6.4176e-04, 1.7714e-03],\n",
      "        ...,\n",
      "        [1.7318e-07, 1.0000e+00, 3.0399e-07, 2.6407e-07, 1.1446e-07, 6.3000e-08],\n",
      "        [2.7117e-05, 8.3975e-04, 7.2033e-05, 4.3379e-05, 1.8516e-05, 9.9900e-01],\n",
      "        [1.1900e-06, 8.5545e-05, 9.9991e-01, 1.7780e-06, 8.3232e-07, 3.4258e-08]])\n",
      "0 143 93 0 0\n",
      "test loss:  tensor(1.3724, dtype=torch.float64)\n",
      "hamming loss: 0.25666666666666665\n",
      "total reward:  tensor(251.2000, dtype=torch.float64)\n",
      "auc:  0.5678032729602236\n",
      "JCP Rev = 251.20\n",
      "(700, 292)\n",
      "(300, 292)\n",
      "{0, 1}\n",
      "Current Rep:  1\n",
      "Dataset :  <torch.utils.data.dataset.TensorDataset object at 0x7ff6756badd8>\n",
      "Number of samples:  700\n",
      "Epoch[0/5000], loss: -0.591852\n",
      "Epoch[1000/5000], loss: -0.940042\n",
      "Epoch[2000/5000], loss: -0.940769\n",
      "Epoch[3000/5000], loss: -0.934463\n",
      "Epoch[4000/5000], loss: -0.924858\n",
      "test loss:  tensor(1.2407, dtype=torch.float64)\n",
      "hamming loss: 0.24\n",
      "total reward:  tensor(228., dtype=torch.float64)\n",
      "auc:  0.7720575388736823\n",
      "IPS = 228.00\n",
      "Epoch[0/2000], loss: -0.881040\n",
      "Epoch[1000/2000], loss: -0.943897\n",
      "fraction:\n",
      "tensor(0.9833)\n",
      "Pure Alg Reward:\n",
      "tensor(228., dtype=torch.float64)\n",
      "Pure Human Reward:\n",
      "tensor(237.9999)\n",
      "test loss:  tensor(1.2407, dtype=torch.float64)\n",
      "hamming loss: 0.24\n",
      "total reward:  tensor(228.7500)\n",
      "auc:  0.7720575388736823\n",
      "TS Rev = 228.75\n",
      "Epoch[0/5000], loss: -0.558005\n",
      "Epoch[1000/5000], loss: -0.907945\n",
      "Epoch[2000/5000], loss: -0.903572\n",
      "Epoch[3000/5000], loss: -0.909076\n",
      "Epoch[4000/5000], loss: -0.905938\n",
      "fraction:\n",
      "tensor(0.7667)\n",
      "Pure Alg Reward:\n",
      "tensor(223., dtype=torch.float64)\n",
      "Pure Human Reward:\n",
      "tensor(237.9999)\n",
      "test loss:  tensor(1.5588, dtype=torch.float64)\n",
      "hamming loss: 0.25666666666666665\n",
      "total reward:  tensor(251.5000)\n",
      "auc:  0.6160532292819288\n",
      "JC Rev = 251.50\n",
      "Epoch[0/5000], loss: -0.792033\n",
      "Epoch[1000/5000], loss: -1.062955\n",
      "Epoch[2000/5000], loss: -1.082359\n",
      "Epoch[3000/5000], loss: -1.120934\n",
      "Epoch[4000/5000], loss: -1.107271\n",
      "tensor([[8.5691e-03, 9.6397e-01, 2.5371e-02, 9.2658e-04, 6.8099e-04, 4.8728e-04],\n",
      "        [8.7401e-02, 2.0523e-01, 1.1841e-01, 7.6497e-03, 5.1891e-03, 5.7612e-01],\n",
      "        [4.9724e-04, 9.9817e-01, 5.8124e-04, 1.5226e-04, 1.0691e-04, 4.9408e-04],\n",
      "        ...,\n",
      "        [7.9926e-04, 9.9687e-01, 3.7592e-04, 1.0006e-04, 6.4035e-05, 1.7872e-03],\n",
      "        [3.4876e-09, 9.0312e-02, 9.0483e-01, 1.7967e-05, 1.0451e-05, 4.8298e-03],\n",
      "        [3.6737e-02, 9.3551e-01, 2.6148e-02, 8.4444e-04, 6.2501e-04, 1.3814e-04]])\n",
      "57 138 43 0 0\n",
      "test loss:  tensor(1.2989, dtype=torch.float64)\n",
      "hamming loss: 0.25666666666666665\n",
      "total reward:  tensor(263.1000, dtype=torch.float64)\n",
      "auc:  0.6162425018927262\n",
      "JCP Rev = 263.10\n",
      "(700, 292)\n",
      "(300, 292)\n",
      "{0, 1}\n",
      "Current Rep:  2\n",
      "Dataset :  <torch.utils.data.dataset.TensorDataset object at 0x7ff6756ac5c0>\n",
      "Number of samples:  700\n",
      "Epoch[0/5000], loss: -0.528416\n",
      "Epoch[1000/5000], loss: -0.924641\n",
      "Epoch[2000/5000], loss: -0.923689\n",
      "Epoch[3000/5000], loss: -0.928100\n",
      "Epoch[4000/5000], loss: -0.926910\n",
      "test loss:  tensor(0.9531, dtype=torch.float64)\n",
      "hamming loss: 0.20333333333333334\n",
      "total reward:  tensor(239., dtype=torch.float64)\n",
      "auc:  0.8045972151756018\n",
      "IPS = 239.00\n",
      "Epoch[0/2000], loss: -0.875452\n",
      "Epoch[1000/2000], loss: -0.938501\n",
      "fraction:\n",
      "tensor(0.9900)\n",
      "Pure Alg Reward:\n",
      "tensor(239., dtype=torch.float64)\n",
      "Pure Human Reward:\n",
      "tensor(232.9999)\n",
      "test loss:  tensor(0.9531, dtype=torch.float64)\n",
      "hamming loss: 0.20333333333333334\n",
      "total reward:  tensor(238.8500)\n",
      "auc:  0.8045972151756018\n",
      "TS Rev = 238.85\n",
      "Epoch[0/5000], loss: -0.549106\n",
      "Epoch[1000/5000], loss: -0.802173\n",
      "Epoch[2000/5000], loss: -0.802175\n",
      "Epoch[3000/5000], loss: -0.799339\n",
      "Epoch[4000/5000], loss: -0.802173\n",
      "fraction:\n",
      "tensor(0.)\n",
      "Pure Alg Reward:\n",
      "tensor(219., dtype=torch.float64)\n",
      "Pure Human Reward:\n",
      "tensor(232.9999)\n",
      "test loss:  tensor(0.6054, dtype=torch.float64)\n",
      "hamming loss: 0.27\n",
      "total reward:  tensor(232.9999)\n",
      "auc:  0.40260161226675695\n",
      "JC Rev = 233.00\n",
      "Epoch[0/5000], loss: -0.773799\n",
      "Epoch[1000/5000], loss: -1.074888\n",
      "Epoch[2000/5000], loss: -1.087257\n",
      "Epoch[3000/5000], loss: -1.137645\n",
      "Epoch[4000/5000], loss: -1.130608\n",
      "tensor([[1.6895e-06, 9.9991e-01, 8.8946e-05, 7.8034e-07, 7.9334e-07, 7.0434e-07],\n",
      "        [4.5926e-09, 1.0000e+00, 4.2604e-12, 1.5037e-08, 2.3728e-09, 2.0490e-09],\n",
      "        [6.7711e-04, 1.6358e-02, 9.4770e-01, 3.4537e-02, 3.7774e-04, 3.5049e-04],\n",
      "        ...,\n",
      "        [2.8245e-05, 9.0722e-01, 9.2727e-02, 2.0163e-07, 1.5215e-05, 1.3975e-05],\n",
      "        [3.5162e-04, 7.4314e-03, 9.8983e-01, 1.9811e-03, 2.0815e-04, 1.9529e-04],\n",
      "        [2.2864e-04, 9.5114e-01, 6.3534e-05, 4.8318e-02, 1.3217e-04, 1.2089e-04]])\n",
      "0 213 40 47 0\n",
      "test loss:  tensor(0.6704, dtype=torch.float64)\n",
      "hamming loss: 0.27\n",
      "total reward:  tensor(263.0000, dtype=torch.float64)\n",
      "auc:  0.5\n",
      "JCP Rev = 263.00\n",
      "(700, 292)\n",
      "(300, 292)\n",
      "{0, 1}\n",
      "Current Rep:  3\n",
      "Dataset :  <torch.utils.data.dataset.TensorDataset object at 0x7ff6755f5fd0>\n",
      "Number of samples:  700\n",
      "Epoch[0/5000], loss: -0.415301\n",
      "Epoch[1000/5000], loss: -0.916703\n",
      "Epoch[2000/5000], loss: -0.927567\n",
      "Epoch[3000/5000], loss: -0.921719\n",
      "Epoch[4000/5000], loss: -0.933255\n",
      "test loss:  tensor(1.0412, dtype=torch.float64)\n",
      "hamming loss: 0.21666666666666667\n",
      "total reward:  tensor(235., dtype=torch.float64)\n",
      "auc:  0.8306370370370371\n",
      "IPS = 235.00\n",
      "Epoch[0/2000], loss: -0.866895\n",
      "Epoch[1000/2000], loss: -0.943969\n",
      "fraction:\n",
      "tensor(0.9533)\n",
      "Pure Alg Reward:\n",
      "tensor(235., dtype=torch.float64)\n",
      "Pure Human Reward:\n",
      "tensor(243.9999)\n",
      "test loss:  tensor(1.0412, dtype=torch.float64)\n",
      "hamming loss: 0.21666666666666667\n",
      "total reward:  tensor(233.3000)\n",
      "auc:  0.8306370370370371\n",
      "TS Rev = 233.30\n",
      "Epoch[0/5000], loss: -0.551191\n",
      "Epoch[1000/5000], loss: -0.899085\n",
      "Epoch[2000/5000], loss: -0.899630\n",
      "Epoch[3000/5000], loss: -0.899370\n",
      "Epoch[4000/5000], loss: -0.899590\n",
      "fraction:\n",
      "tensor(0.7467)\n",
      "Pure Alg Reward:\n",
      "tensor(225., dtype=torch.float64)\n",
      "Pure Human Reward:\n",
      "tensor(243.9999)\n",
      "test loss:  tensor(1.4872, dtype=torch.float64)\n",
      "hamming loss: 0.25\n",
      "total reward:  tensor(247.2000)\n",
      "auc:  0.6542962962962963\n",
      "JC Rev = 247.20\n",
      "Epoch[0/5000], loss: -0.764192\n",
      "Epoch[1000/5000], loss: -1.088225\n",
      "Epoch[2000/5000], loss: -1.056141\n",
      "Epoch[3000/5000], loss: -1.095051\n",
      "Epoch[4000/5000], loss: -1.095613\n",
      "tensor([[2.1096e-09, 1.0000e+00, 1.1903e-06, 5.6169e-08, 3.0802e-08, 1.5088e-06],\n",
      "        [3.7206e-06, 7.2793e-05, 8.1208e-05, 5.6049e-06, 3.3083e-06, 9.9983e-01],\n",
      "        [1.1315e-02, 8.5081e-01, 8.6433e-02, 2.3446e-03, 1.5099e-03, 4.7588e-02],\n",
      "        ...,\n",
      "        [1.3044e-06, 6.9877e-01, 4.4405e-03, 8.4221e-05, 4.9908e-05, 2.9665e-01],\n",
      "        [1.2398e-04, 9.9155e-03, 1.5900e-02, 3.5779e-04, 2.3348e-04, 9.7347e-01],\n",
      "        [3.2921e-02, 5.3813e-01, 2.6328e-02, 4.0300e-03, 2.6603e-03, 3.9594e-01]])\n",
      "33 75 98 0 0\n",
      "test loss:  tensor(1.3295, dtype=torch.float64)\n",
      "hamming loss: 0.25\n",
      "total reward:  tensor(263.7000, dtype=torch.float64)\n",
      "auc:  0.6671555555555555\n",
      "JCP Rev = 263.70\n",
      "(700, 292)\n",
      "(300, 292)\n",
      "{0, 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Rep:  4\n",
      "Dataset :  <torch.utils.data.dataset.TensorDataset object at 0x7ff6756c9c50>\n",
      "Number of samples:  700\n",
      "Epoch[0/5000], loss: -0.613075\n",
      "Epoch[1000/5000], loss: -0.930609\n",
      "Epoch[2000/5000], loss: -0.951893\n",
      "Epoch[3000/5000], loss: -0.942655\n",
      "Epoch[4000/5000], loss: -0.945098\n",
      "test loss:  tensor(1.4736, dtype=torch.float64)\n",
      "hamming loss: 0.24666666666666667\n",
      "total reward:  tensor(226., dtype=torch.float64)\n",
      "auc:  0.7187183062880325\n",
      "IPS = 226.00\n",
      "Epoch[0/2000], loss: -0.880306\n",
      "Epoch[1000/2000], loss: -0.953990\n",
      "fraction:\n",
      "tensor(0.9900)\n",
      "Pure Alg Reward:\n",
      "tensor(226., dtype=torch.float64)\n",
      "Pure Human Reward:\n",
      "tensor(227.9999)\n",
      "test loss:  tensor(1.4736, dtype=torch.float64)\n",
      "hamming loss: 0.24666666666666667\n",
      "total reward:  tensor(224.8500)\n",
      "auc:  0.7187183062880325\n",
      "TS Rev = 224.85\n",
      "Epoch[0/5000], loss: -0.561402\n",
      "Epoch[1000/5000], loss: -0.915542\n",
      "Epoch[2000/5000], loss: -0.909702\n",
      "Epoch[3000/5000], loss: -0.913887\n",
      "Epoch[4000/5000], loss: -0.921799\n",
      "fraction:\n",
      "tensor(0.7133)\n",
      "Pure Alg Reward:\n",
      "tensor(232., dtype=torch.float64)\n",
      "Pure Human Reward:\n",
      "tensor(227.9999)\n",
      "test loss:  tensor(1.3420, dtype=torch.float64)\n",
      "hamming loss: 0.22666666666666666\n",
      "total reward:  tensor(241.7000)\n",
      "auc:  0.6526844574036512\n",
      "JC Rev = 241.70\n",
      "Epoch[0/5000], loss: -0.780515\n",
      "Epoch[1000/5000], loss: -1.108240\n",
      "Epoch[2000/5000], loss: -1.089666\n",
      "Epoch[3000/5000], loss: -1.065469\n",
      "Epoch[4000/5000], loss: -1.083496\n",
      "tensor([[4.2120e-01, 5.5230e-05, 5.7864e-01, 5.9676e-05, 4.3210e-05, 5.0045e-06],\n",
      "        [5.5408e-01, 2.5508e-04, 5.9139e-06, 2.5576e-03, 2.0329e-04, 4.4289e-01],\n",
      "        [9.5692e-07, 1.5693e-05, 1.7833e-02, 7.1880e-05, 1.2081e-05, 9.8207e-01],\n",
      "        ...,\n",
      "        [1.0211e-03, 2.6926e-05, 2.7020e-07, 3.2965e-03, 2.1130e-05, 9.9563e-01],\n",
      "        [2.9173e-04, 1.7092e-05, 1.4227e-03, 9.9825e-01, 1.3923e-05, 9.3815e-08],\n",
      "        [3.0025e-04, 8.2739e-05, 9.8801e-01, 2.9523e-05, 6.6040e-05, 1.1509e-02]])\n",
      "48 0 49 102 0\n",
      "test loss:  tensor(1.2436, dtype=torch.float64)\n",
      "hamming loss: 0.22666666666666666\n",
      "total reward:  tensor(254.0500, dtype=torch.float64)\n",
      "auc:  0.649340770791075\n",
      "JCP Rev = 254.05\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "from scipy.special import expit\n",
    "from random import seed\n",
    "from random import randrange\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "import argparse \n",
    "import torch.nn as nn \n",
    "import random\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from skmultilearn.dataset import load_dataset\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import hamming_loss, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time \n",
    "\n",
    "\n",
    "nrep = 5\n",
    "dataset = 'real_focus'\n",
    "hidd = 2\n",
    "rand_test = 0\n",
    "C = 0.05\n",
    "\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes, hidden = 128):\n",
    "        super(Net, self).__init__()\n",
    "        self.norm0 = torch.nn.BatchNorm1d(input_size)\n",
    "        self.linear1 = torch.nn.Linear(input_size, hidden)\n",
    "        self.linear2 = torch.nn.Linear(hidden, num_classes)\n",
    "        self.drop_layer = nn.Dropout(p=0.2)\n",
    "\n",
    "    def emb(self, x):\n",
    "        emb = self.linear1(x)\n",
    "        return emb\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        hidd1 = self.linear1(x)\n",
    "        out = self.linear2(hidd1)\n",
    "        out = self.drop_layer(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class SelNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes, hidden = 128):\n",
    "        super(SelNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_size, hidden)\n",
    "        self.linear2 = torch.nn.Linear(hidden, num_classes)\n",
    "\n",
    "    def emb(self, x):\n",
    "        emb = self.linear1(x)\n",
    "        return emb\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidd1 = F.relu(self.linear1(x))\n",
    "        out = self.linear2(hidd1)\n",
    "        return out\n",
    "\n",
    "def find_instance(w):\n",
    "    # input worker id \n",
    "    # return the instances worker labeled\n",
    "    return np.where(human[w][0].any(axis=1))[0]\n",
    "\n",
    "def find_worker(ins):\n",
    "    # input instance id \n",
    "    # return which workers labeled it\n",
    "    ws = []\n",
    "    for w in range(18):\n",
    "        if ins in  np.where(human[w][0].any(axis=1))[0]:\n",
    "            ws.append(w)\n",
    "    return np.array(ws)\n",
    "\n",
    "\n",
    "\n",
    "def find_unlabeled(human):\n",
    "    inss = []\n",
    "    for ins in range(700):\n",
    "        ws = []\n",
    "        for w in range(18):\n",
    "            if ins in  np.where(human[w][0].any(axis=1))[0]:\n",
    "                ws.append(w)\n",
    "        if len(ws) == 0:\n",
    "            inss.append(ins)\n",
    "    return np.array(inss)\n",
    "\n",
    "\n",
    "\n",
    "@ torch.no_grad()\n",
    "def test_multi(model, X_test, y_test):\n",
    "    set_seed(999)\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    out1 = model(X_test.float())\n",
    "    out = torch.sigmoid(out1)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "    _, treat = torch.max(out, 1)\n",
    "    #y = sample_y(X_test, hx_test, np.array(opt), dataset)\n",
    "    labels = y_test[range(y_test.shape[0]),treat.numpy()]\n",
    "    total_reward = labels.reshape(-1).sum()\n",
    "    pred = torch.where(out<0.5, torch.zeros_like(out), torch.ones_like(out))\n",
    "    ham_loss = hamming_loss(y_test.numpy(), pred.detach().numpy())\n",
    "    auc = roc_auc_score(y_test.numpy(), out.detach().numpy())\n",
    "    print('test loss: ', torch.nn.MultiLabelSoftMarginLoss()(out1, y_test))\n",
    "    print('hamming loss:', hamming_loss(y_test.numpy(), pred.detach().numpy()))\n",
    "    print('total reward: ', total_reward)\n",
    "    print('auc: ', auc)\n",
    "    model.train()\n",
    "    return total_reward, ham_loss, auc\n",
    "\n",
    "\n",
    "@ torch.no_grad()\n",
    "def test_multi_human(model, X_test, y_test, human, selector):\n",
    "    set_seed(999)\n",
    "    model.cpu()\n",
    "    human.cpu()\n",
    "    selector.cpu()\n",
    "\n",
    "    out1 = model(X_test.float())\n",
    "    out = torch.sigmoid(out1)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "    _, treat = torch.max(out, 1)\n",
    "\n",
    "    pred = out[range(y_test.shape[0]),treat]\n",
    "    htreat = np.argmax(human(X_test.float()).detach().cpu().numpy(),1)\n",
    "    htreat = torch.from_numpy(htreat)\n",
    "    if_alg = selector(X_test.float())\n",
    "    if_alg = torch.sigmoid(if_alg)[range(y_test.shape[0]),[0]*y_test.shape[0]]\n",
    "    treat = torch.where(if_alg > 0.5, treat, htreat)\n",
    "    #y = sample_y(X_test, hx_test, np.array(opt), dataset)\n",
    "    labels = y_test[range(y_test.shape[0]),treat.numpy()]\n",
    "    total_reward = labels.reshape(-1).sum()\n",
    "\n",
    "    pred = torch.where(out<0.5, torch.zeros_like(out), torch.ones_like(out))\n",
    "\n",
    "    ham_loss = hamming_loss(y_test.numpy(), pred.detach().numpy())\n",
    "    auc = roc_auc_score(y_test.numpy(), out.detach().numpy())\n",
    "\n",
    "    print('test loss: ', torch.nn.MultiLabelSoftMarginLoss()(out1, y_test))\n",
    "    print('hamming loss:', hamming_loss(y_test.numpy(), pred.detach().numpy()))\n",
    "    print('total reward: ', total_reward)\n",
    "    print('auc: ', auc)\n",
    "    return total_reward, ham_loss, auc\n",
    "\n",
    "@ torch.no_grad()\n",
    "def test_real_human(model, X_test, y_test, human, selector, C=0):\n",
    "    set_seed(999)\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    selector.cpu()\n",
    "\n",
    "    out1 = model(X_test.float())\n",
    "    out = torch.sigmoid(out1)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "    _, treat = torch.max(out, 1)\n",
    "    alg_treat = treat\n",
    "\n",
    "    pred = out[range(y_test.shape[0]),treat]\n",
    "    pred = out[range(y_test.shape[0]),treat]\n",
    "\n",
    "    treatment = []\n",
    "    for ins in indices_test:\n",
    "        #ws = np.array([9, 3, 11])\n",
    "        ws = np.array([0, 1, 2,3,4])\n",
    "        # randomly sample a worker\n",
    "        w = np.random.choice(ws)\n",
    "        full = human[w][ins]\n",
    "        treatment.append(randargmax(full))\n",
    "\n",
    "    htreat = np.array(treatment)\n",
    "\n",
    "    htreat = torch.from_numpy(htreat)\n",
    "    if_alg = selector(X_test.float())\n",
    "    if_alg = torch.sigmoid(if_alg)[range(y_test.shape[0]),[0]*y_test.shape[0]]\n",
    "    print('fraction:')\n",
    "    print((if_alg>0.5).float().mean())\n",
    "    treat = torch.where(if_alg > 0.5, treat, htreat)\n",
    "\n",
    "    #y = sample_y(X_test, hx_test, np.array(opt), dataset)\n",
    "    labels = y_test[range(y_test.shape[0]),treat.numpy()]\n",
    "    labels = labels.float()\n",
    "    this_reward = labels.reshape(-1)\n",
    "    this_reward = torch.where(if_alg > 0.5, labels, labels-C)\n",
    "    total_reward = this_reward.reshape(-1).sum()\n",
    "\n",
    "    print('Pure Alg Reward:')\n",
    "    labels = y_test[range(y_test.shape[0]),alg_treat.numpy()]\n",
    "    alg_reward = labels.reshape(-1).sum()\n",
    "    print(alg_reward)\n",
    "\n",
    "    print('Pure Human Reward:')\n",
    "    labels = y_test[range(y_test.shape[0]),htreat.numpy()]\n",
    "    labels = labels.float()\n",
    "    labels = labels - C\n",
    "    human_reward = labels.reshape(-1).sum()\n",
    "    print(human_reward)\n",
    "\n",
    "    pred = torch.where(out<0.5, torch.zeros_like(out), torch.ones_like(out))\n",
    "\n",
    "    ham_loss = hamming_loss(y_test.numpy(), pred.detach().numpy())\n",
    "    auc = roc_auc_score(y_test.numpy(), out.detach().numpy())\n",
    "\n",
    "    print('test loss: ', torch.nn.MultiLabelSoftMarginLoss()(out1, y_test))\n",
    "    print('hamming loss:', hamming_loss(y_test.numpy(), pred.detach().numpy()))\n",
    "    print('total reward: ', total_reward)\n",
    "    print('auc: ', auc)\n",
    "    model.train()\n",
    "    return total_reward, ham_loss, auc, human_reward\n",
    "\n",
    "@ torch.no_grad()\n",
    "def test_real_human_person(model, X_test, y_test, human, selector, C = 0):\n",
    "    set_seed(999)\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    selector.cpu()\n",
    "\n",
    "    out1 = model(X_test.float())\n",
    "    out = torch.sigmoid(out1)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "    _, treat = torch.max(out, 1)\n",
    "\n",
    "    pred = out[range(y_test.shape[0]),treat]\n",
    "    pred = out[range(y_test.shape[0]),treat]\n",
    "\n",
    "\n",
    "    if_alg = selector(X_test.float())\n",
    "    if_alg = F.softmax(if_alg, 1)\n",
    "\n",
    "    print(if_alg)\n",
    "    _, selection = torch.max(if_alg, 1)\n",
    "\n",
    "    ma = {0:0,1:1,2:2,3:3,4:4}\n",
    "    final_treat = []\n",
    "\n",
    "    treatment = []\n",
    "    treat = treat.numpy()\n",
    "    human_cost = 0 \n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    count3 = 0\n",
    "    count4 = 0\n",
    "    count5 = 0\n",
    "    for ind, ins in enumerate(indices_test):\n",
    "        i = selection[ind]\n",
    "        if i == 0:\n",
    "            fulllabel = human[0][ins]\n",
    "            treatment.append(randargmax(fulllabel))\n",
    "            human_cost += C\n",
    "            count1 += 1\n",
    "        if i == 1:\n",
    "            fulllabel = human[1][ins]\n",
    "            treatment.append(randargmax(fulllabel))\n",
    "            human_cost += C\n",
    "            count2 += 1\n",
    "        if i == 2:\n",
    "            fulllabel = human[2][ins]\n",
    "            treatment.append(randargmax(fulllabel))  \n",
    "            human_cost += C                 \n",
    "            count3 += 1\n",
    "        if i == 3:\n",
    "            fulllabel = human[3][ins]\n",
    "            treatment.append(randargmax(fulllabel))  \n",
    "            human_cost += C                 \n",
    "            count4 += 1\n",
    "        if i == 4:\n",
    "            fulllabel = human[4][ins]\n",
    "            treatment.append(randargmax(fulllabel))  \n",
    "            human_cost += C                 \n",
    "            count5 += 1\n",
    "        if i == 5:\n",
    "            treatment.append(treat[ind])                  \n",
    "    print(count1, count2, count3, count4, count5)\n",
    "    treat = np.array(treatment)\n",
    "    #y = sample_y(X_test, hx_test, np.array(opt), dataset)\n",
    "    labels = y_test[range(y_test.shape[0]),treat]\n",
    "    total_reward = labels.reshape(-1).sum() - human_cost\n",
    "\n",
    "    pred = torch.where(out<0.5, torch.zeros_like(out), torch.ones_like(out))\n",
    "\n",
    "    ham_loss = hamming_loss(y_test.numpy(), pred.detach().numpy())\n",
    "    auc = roc_auc_score(y_test.numpy(), out.detach().numpy())\n",
    "\n",
    "    print('test loss: ', torch.nn.MultiLabelSoftMarginLoss()(out1, y_test))\n",
    "    print('hamming loss:', hamming_loss(y_test.numpy(), pred.detach().numpy()))\n",
    "    print('total reward: ', total_reward)\n",
    "    print('auc: ', auc)\n",
    "    model.train()\n",
    "    return total_reward, ham_loss, auc\n",
    "\n",
    "\n",
    "\n",
    "def train_ips(trainloader, input_dim, output_dim, lamb = 0):\n",
    "    model = Net(input_dim, output_dim, hidden = hidd)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    cmcriterion = torch.nn.MultiLabelSoftMarginLoss()\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr = 1e-1, momentum=0.9, weight_decay = 1e-4)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 1e-4)\n",
    "    num_epochs = 5000\n",
    "    min_loss = 1e3\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (inputx, treatment, targety, logging_prob) in enumerate(trainloader):\n",
    "            # forward\n",
    "            out = model(inputx.float())\n",
    "            out = F.softmax(out, 1)\n",
    "            out = out[range(out.size(0)),treatment]\n",
    "            logp = logging_prob[range(out.size(0)),treatment]\n",
    "            reward = targety.reshape(-1)\n",
    "            loss = - (reward - lamb) * out / logp\n",
    "            loss = loss.mean()\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        if epoch % 1000 == 0:\n",
    "            print('Epoch[{}/{}], loss: {:.6f}'.format(epoch, num_epochs, running_loss / (batch_idx+1)))\n",
    "    return model\n",
    "\n",
    "def train_ips_2s(trainloader, input_dim, output_dim, model, lamb = 0, C = 0):\n",
    "    model = model\n",
    "    #model = Net(input_dim, output_dim, hidden = hidd)\n",
    "    selector = SelNet(input_dim, 1, hidden = 16)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    cmcriterion = torch.nn.MultiLabelSoftMarginLoss()\n",
    "    #optimizer = torch.optim.SGD(selector.parameters(), lr = 1e-2, momentum=0.9, weight_decay = 1e-4)\n",
    "    optimizer = torch.optim.Adam(selector.parameters(), lr = 1e-3, weight_decay = 1e-4)\n",
    "    num_epochs = 2000\n",
    "    min_loss = 1e3\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (inputx, treatment, targety, logging_prob) in enumerate(trainloader):\n",
    "            # forward\n",
    "            with torch.no_grad():\n",
    "                out = model(inputx.float())\n",
    "                out = F.softmax(out, 1)\n",
    "                out = out[range(out.size(0)),treatment]\n",
    "            # prob that selcts alg\n",
    "            sel_prob = selector(inputx.float())\n",
    "            sel_prob = torch.sigmoid(sel_prob).reshape(-1)\n",
    "            #print('prob')\n",
    "            #print(sel_prob)\n",
    "            logp = logging_prob[range(out.size(0)),treatment]\n",
    "            reward = targety.reshape(-1)\n",
    "            loss = - (reward - lamb) * (sel_prob * out / logp) - (1-sel_prob) * (reward - lamb - C)\n",
    "            '''\n",
    "            print('sel_prob')\n",
    "            print(sel_prob)\n",
    "            print('alg_reward')\n",
    "            print(reward *  out / logp)\n",
    "            print('human reward')\n",
    "            print(reward)\n",
    "            '''\n",
    "            loss = loss.mean()\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        if epoch % 1000 == 0:\n",
    "            print('Epoch[{}/{}], loss: {:.6f}'.format(epoch, num_epochs, running_loss / (batch_idx+1)))\n",
    "    return model, selector\n",
    "\n",
    "\n",
    "def train_ips_hai(trainloader, model, selector, input_dim, output_dim, lamb = 0, C = 0):\n",
    "    model = Net(input_dim, output_dim, hidden = hidd)\n",
    "    #selector = SelNet(input_dim, 1, hidden = 16)\n",
    "    #model = model\n",
    "    selector = selector\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    cmcriterion = torch.nn.MultiLabelSoftMarginLoss()\n",
    "    optimizer = torch.optim.Adam(list(model.parameters())+list(selector.parameters()), lr = 1e-3, weight_decay = 1e-4)\n",
    "    #optimizer = torch.optim.SGD(list(model.parameters())+list(selector.parameters()), lr = 1e-4, momentum = 0.9, weight_decay = 1e-4)\n",
    "    #optimizer = torch.optim.SGD(list(model.parameters())+list(selector.parameters()), lr = 1e-3, momentum=0.9, weight_decay = 1e-4)\n",
    "    #model_optimizer = torch.optim.SGD(list(model.parameters()), lr = 0.01, momentum=0.9, weight_decay = 1e-4)\n",
    "    #sel_optimizer = torch.optim.Adam(list(selector.parameters()), lr = 0.01, weight_decay = 1e-4)\n",
    "    num_epochs = 5000\n",
    "    min_loss = 1e3\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (inputx, treatment, targety, logging_prob) in enumerate(trainloader):\n",
    "            # forward\n",
    "            out = model(inputx.float())\n",
    "            out = F.softmax(out, 1)\n",
    "            out = out[range(out.size(0)),treatment]\n",
    "            # prob that selcts alg\n",
    "            sel_prob = selector(inputx.float())\n",
    "            sel_prob = torch.sigmoid(sel_prob).reshape(-1)\n",
    "            #print('prob')\n",
    "            #print(sel_prob)\n",
    "            logp = logging_prob[range(out.size(0)),treatment]\n",
    "            reward = targety.reshape(-1)\n",
    "            loss = - (reward - lamb) * (sel_prob * out / logp) - (1-sel_prob) * (reward - lamb - C)\n",
    "            loss = loss.mean()\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        if epoch % 1000 == 0:\n",
    "            print('Epoch[{}/{}], loss: {:.6f}'.format(epoch, num_epochs, running_loss / (batch_idx+1)))\n",
    "    return model, selector\n",
    "\n",
    "\n",
    "def train_ips_hai_person(trainloader, model, input_dim, output_dim, lamb = 0, C = 0):\n",
    "    #model = Net(input_dim, output_dim, hidden = hidd)\n",
    "    model = model\n",
    "    selector = SelNet(input_dim, 6, hidden = 16)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    cmcriterion = torch.nn.MultiLabelSoftMarginLoss()\n",
    "    optimizer = torch.optim.Adam(list(model.parameters())+list(selector.parameters()), lr = 1e-3,weight_decay=1e-4)\n",
    "    #optimizer = torch.optim.SGD(list(model.parameters())+list(selector.parameters()), lr = 1e-1, momentum=0.9, weight_decay = 1e-4)\n",
    "    #temp_optimizer = torch.optim.SGD([temperature], lr=0.001, momentum=0.9, weight_decay = 1e-4)\n",
    "    num_epochs = 5000\n",
    "    min_loss = 1e3\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (inputx, treatment, targety, logging_prob, prop, wids) in enumerate(trainloader):            # forward\n",
    "            out = model(inputx.float())\n",
    "            out = F.softmax(out, 1)\n",
    "            out = out[range(out.size(0)),treatment]\n",
    "            # prob that selcts alg\n",
    "            sel_prob = selector(inputx.float())\n",
    "            sel_prob = F.softmax(sel_prob, 1)\n",
    "            \n",
    "            logp = logging_prob\n",
    "            #logp = logging_prob[range(out.size(0)),treatment]\n",
    "            #logp = torch.sum(prop, 1) * 1 / 5\n",
    "            reward = targety.reshape(-1)\n",
    "            loss = - (reward - lamb) * (sel_prob[:,5] * out / logp) - (reward - lamb - C) * 5 * sel_prob[:,wids] \n",
    "            loss = loss.mean()\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        if epoch % 1000 == 0:\n",
    "            print('Epoch[{}/{}], loss: {:.6f}'.format(epoch, num_epochs, running_loss / (batch_idx+1)))   \n",
    "    return model, selector\n",
    "\n",
    "def randargmax(b,**kw):\n",
    "  \"\"\" a random tie-breaking argmax\"\"\"\n",
    "  return np.argmax(np.random.random(b.shape) * (b==b.max()), **kw)\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "opt_result = []\n",
    "ips_result = []\n",
    "ips_loss = []\n",
    "ips_time = []\n",
    "ips_auc = []\n",
    "ips2s_result = []\n",
    "ips2s_loss = []\n",
    "ips2s_time = []\n",
    "ips2s_auc = []\n",
    "ipsj_result = []\n",
    "ipsj_loss = []\n",
    "ipsj_time = []\n",
    "ipsj_auc = []\n",
    "ipsjp_result = []\n",
    "ipsjp_loss = []\n",
    "ipsjp_time = []\n",
    "ipsjp_auc = []\n",
    "human_result = []\n",
    "\n",
    "lamb = 0.\n",
    "#C = 0\n",
    "dataset0 = dataset\n",
    "\n",
    "for r in range(nrep):\n",
    "\n",
    "    set_seed(r)\n",
    "\n",
    "    if dataset0 == 'human':\n",
    "        '''\n",
    "        selAnn:      crowds annotations, [numWorker,1] cell, one cell for one worker's annotation, [numInst, numLab] matrix. range in {1(positive annotation), -1(negative annotation), 0(no annotation)}  \n",
    "        selGT:       groundtruth labels,  [numInst, numLab] matrix. range in {1(positive), -1(negative)}  \n",
    "        selfvFeat:   feature representation of data, [numFeat,numInst] matrix\n",
    "        trainPairs:  label pairs constructed for datasets \n",
    "        trainPairsW: label pairs constructed for each worker\n",
    "        foldWorker:   folds of worker sequence permutation, [numFold, numWorker]\n",
    "        '''\n",
    "        from scipy.io import loadmat\n",
    "        data = loadmat('./data/data_sample/dataset1/selDatanp.mat')\n",
    "        X = data['selfvFeat'].T\n",
    "        y = data['selGT']\n",
    "        # need to find instances labeled by all labelers\n",
    "        # workers labeled most instances: 9, 3, 11, 15, 12\n",
    "        # choose 3 workers 9, 3, 11\n",
    "\n",
    "        human = data['selAnn']\n",
    "        #instances = np.array(list(set(find_instance(9))&set(find_instance(3))&set(find_instance(11))))\n",
    "        instances = np.array(list(set(find_instance(0))&set(find_instance(7))&set(find_instance(13))))\n",
    "        for w in range(18):\n",
    "            human[w][0] = human[w][0][instances]\n",
    "        X = X[instances]\n",
    "        y = y[instances]\n",
    "        y = y / 2 + 0.5 \n",
    "        print(y)\n",
    "        X, X_test, y, y_test, indices_train, indices_test = train_test_split(X, y, range(len(instances)), test_size=0.1, random_state =  r)\n",
    "        print(X.shape)\n",
    "        print(X_test.shape)\n",
    "        y_all = y\n",
    "        X_test = torch.from_numpy(X_test)\n",
    "    if dataset0 == 'real_focus':\n",
    "        data = pd.read_csv('/home/ruijiang/E/utaustin/project/cost_efficient_labeling/Quick-and-Dirty/data/real/data.csv')\n",
    "        annotation = pd.read_csv('/home/ruijiang/E/utaustin/project/cost_efficient_labeling/Quick-and-Dirty/data/real/focus.csv')\n",
    "        data = data.iloc[:,4:]\n",
    "        X = data.values\n",
    "        y = annotation['gt'].values\n",
    "        a = np.zeros((X.shape[0],2))\n",
    "        a[range(a.shape[0]),y] = 1\n",
    "        y = a\n",
    "        human = dict()\n",
    "        for w in range(5):\n",
    "            temp = np.array(annotation[str(w)])\n",
    "            anno = np.zeros((X.shape[0],2))\n",
    "            anno[range(anno.shape[0]),temp] = 1\n",
    "            human[w] = anno\n",
    "        X, X_test, y, y_test, indices_train, indices_test = train_test_split(X, y, range(X.shape[0]), test_size=0.3, random_state =  r)\n",
    "        print(X.shape)\n",
    "        print(X_test.shape)\n",
    "        y_all = y\n",
    "        X_test = torch.from_numpy(X_test)\n",
    "        ws = range(5)\n",
    "    # human generate the logging policy\n",
    "    np.random.seed(r + 81)\n",
    "    treatment = []\n",
    "    wids = []\n",
    "    for ins in indices_train:\n",
    "        # randomly sample a worker\n",
    "        w = np.random.choice(ws)\n",
    "        wids.append(w)\n",
    "        full = human[w][ins]\n",
    "        treatment.append(randargmax(full))\n",
    "    treatment = np.array(treatment)\n",
    "    wids = np.array(wids)\n",
    "    #m = {3:0,9:1,11:2}\n",
    "    m = {0:0,1:1,2:2,3:3,4:4}\n",
    "    wids_map = np.array([m[i] for i in wids])\n",
    "    # record instances recorded by )\n",
    "    wids_oh = np.zeros((wids.size, wids_map.max()+1))\n",
    "    wids_oh[np.arange(wids.size),wids_map] = 1\n",
    "    y = y_all[range(y_all.shape[0]), treatment]\n",
    "\n",
    "    print(set(treatment))\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    # for ips baseline:\n",
    "    # learn logging policy \n",
    "    Xw = np.append(X,wids_oh,axis=1)\n",
    "    #log_est = LogisticRegression(random_state=0).fit(Xw, treatment)\n",
    "    #log_est0 = LogisticRegression(random_state=0).fit(X, treatment)\n",
    "    log_est = RandomForestClassifier(random_state=0).fit(Xw, treatment)\n",
    "    #log_est = MLPClassifier(random_state=0, max_iter=10000).fit(Xw, treatment)\n",
    "    #log_est = MLPClassifier(random_state=0,max_iter = 10000).fit(X, treatment)\n",
    "    log_est0 = RandomForestClassifier(random_state=0).fit(X, treatment)\n",
    "    #log_est0 = MLPClassifier(random_state=0, max_iter=10000).fit(X, treatment)\n",
    "\n",
    "\n",
    "    logging_prob = log_est0.predict_proba(X)\n",
    "    logging_prob = torch.from_numpy(logging_prob)\n",
    "\n",
    "    X = torch.from_numpy(X)\n",
    "    treatment = torch.from_numpy(treatment)\n",
    "\n",
    "    print('Current Rep: ', r)\n",
    "    print('Dataset : ', dataset)\n",
    "    print('Number of samples: ', X.shape[0])\n",
    "    #model = generate_logging_policy(X, y, frac = 0.05)\n",
    "\n",
    "    dataset = TensorDataset(X, treatment, y, logging_prob)\n",
    "    trainloader = DataLoader(dataset, batch_size = 8, shuffle = True)\n",
    "    \n",
    "    # Naive IPS\n",
    "    start = time.time()\n",
    "    ips = train_ips(trainloader, X.shape[1], y_all.shape[1], lamb = lamb)\n",
    "    end = time.time()\n",
    "    rev, _, _ = test_multi(ips, X_test, y_test)\n",
    "    print('IPS = %.2f' % rev)\n",
    "    ips_result.append(rev)\n",
    "    ips_fix = ips\n",
    "    \n",
    "    start = time.time()\n",
    "    ips, selector = train_ips_2s(trainloader, X.shape[1], y_all.shape[1], ips, lamb = lamb, C = C)\n",
    "    end = time.time()\n",
    "    rev, _, _, hreward = test_real_human(ips, X_test, y_test, human, selector, C = C)\n",
    "    print('TS Rev = %.2f' % rev)\n",
    "    ips2s_result.append(rev)\n",
    "    human_result.append(hreward)\n",
    "    \n",
    "    start = time.time()\n",
    "    ips, selector = train_ips_hai(trainloader, ips, selector, X.shape[1], y_all.shape[1], lamb = lamb, C = C)\n",
    "    end = time.time()\n",
    "    rev, _, _, _ = test_real_human(ips, X_test, y_test, human, selector, C = C)\n",
    "    print('JC Rev = %.2f' % rev)\n",
    "    ipsj_result.append(rev)\n",
    "    \n",
    "    X = X.numpy()\n",
    "    Xw = np.append(X,wids_oh,axis=1)\n",
    "    logging_prob = log_est.predict_proba(Xw)\n",
    "    logging_prob = torch.from_numpy(logging_prob)\n",
    "    X = torch.from_numpy(X)\n",
    "    w1 = np.zeros((wids.size, wids_map.max()+1))\n",
    "    w1[:,0] = 1\n",
    "    w1 = torch.from_numpy(w1)\n",
    "    w2 = np.zeros((wids.size, wids_map.max()+1))\n",
    "    w2[:,1] = 1\n",
    "    w2 = torch.from_numpy(w2)\n",
    "    w3 = np.zeros((wids.size, wids_map.max()+1))\n",
    "    w3[:,2] = 1\n",
    "    w3 = torch.from_numpy(w3)\n",
    "    w4 = np.zeros((wids.size, wids_map.max()+1))\n",
    "    w4[:,3] = 1\n",
    "    w4 = torch.from_numpy(w4)\n",
    "    w5 = np.zeros((wids.size, wids_map.max()+1))\n",
    "    w5[:,4] = 1\n",
    "    w5 = torch.from_numpy(w5)\n",
    "    prop = torch.from_numpy(np.stack([log_est.predict_proba(np.append(X,w1,axis=1))[np.arange(X.shape[0]),treatment],log_est.predict_proba(np.append(X,w2,axis=1))[np.arange(X.shape[0]),treatment],\n",
    "        log_est.predict_proba(np.append(X,w3,axis=1))[np.arange(X.shape[0]),treatment],log_est.predict_proba(np.append(X,w4,axis=1))[np.arange(X.shape[0]),treatment],log_est.predict_proba(np.append(X,w5,axis=1))[np.arange(X.shape[0]),treatment]],axis=1))\n",
    "    logging_prob = prop[range(prop.shape[0]),wids_map]\n",
    "    wids_map = torch.from_numpy(wids_map)\n",
    "    dataset = TensorDataset(X, treatment, y, logging_prob, prop, wids_map)\n",
    "    trainloader = DataLoader(dataset, batch_size = 8, shuffle = True)\n",
    "    \n",
    "    start = time.time()\n",
    "    ips, selector = train_ips_hai_person(trainloader, ips, X.shape[1], y_all.shape[1], lamb = lamb, C = C)\n",
    "    end = time.time()\n",
    "    rev, _, _ = test_real_human_person(ips, X_test, y_test, human, selector, C = C)\n",
    "    print('JCP Rev = %.2f' % rev)\n",
    "    ipsjp_result.append(rev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN:\n",
      "234.99995\n",
      "5.5136194\n",
      "IPS:\n",
      "229.4\n",
      "3.131772660970141\n",
      "IPS-2S:\n",
      "228.95\n",
      "3.0514590400263524\n",
      "IPS-J:\n",
      "242.86996\n",
      "2.7978486667321816\n",
      "IPS-J-P:\n",
      "259.01\n",
      "2.368476303449118\n"
     ]
    }
   ],
   "source": [
    "print('HUMAN:')\n",
    "print(np.mean(human_result))\n",
    "print(np.std(human_result))\n",
    "print('IPS:')\n",
    "print(np.mean(ips_result))\n",
    "print(np.std(ips_result)/np.sqrt(len(ips_result)))\n",
    "print('IPS-2S:')\n",
    "print(np.mean(ips2s_result))\n",
    "print(np.std(ips2s_result)/np.sqrt(len(ips2s_result)))\n",
    "print('IPS-J:')\n",
    "print(np.mean(ipsj_result))\n",
    "print(np.std(ipsj_result)/np.sqrt(len(ipsj_result)))\n",
    "print('IPS-J-P:')\n",
    "print(np.mean(ipsjp_result))\n",
    "print(np.std(ipsjp_result)/np.sqrt(len(ipsjp_result)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
